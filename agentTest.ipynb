{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-14T06:44:13.743319Z",
     "start_time": "2025-07-14T06:44:12.053528Z"
    }
   },
   "source": [
    "from gettext import textdomain\n",
    "\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "\n",
    "chat = ChatZhipuAI();\n",
    "\n",
    "print(chat.invoke(\"‰Ω†Â•Ω\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©ÊâãÊô∫Ë∞±Ê∏ÖË®ÄÔºåÂèØ‰ª•Âè´ÊàëÂ∞èÊô∫ü§ñÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 6, 'total_tokens': 38}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run--fc07df51-8c3f-4d42-89fc-8684b75f19a5-0'\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cefe1524385b6218"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T07:13:54.669828Z",
     "start_time": "2025-07-14T07:13:41.190296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "from langchain_core.prompts import PromptTemplate, HumanMessagePromptTemplate,SystemMessagePromptTemplate,AIMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage,HumanMessage,AIMessage\n",
    "\n",
    "chat = ChatZhipuAI(model=\"glm-4\",temperature=1.0);\n",
    "\n",
    "human_promp = HumanMessagePromptTemplate.from_template(\"ËØ∑ËÆ≤‰∏Ä‰∏™ÂÖ≥‰∫é{topic}ÁöÑÊÅêÊÄñÊïÖ‰∫ã\").format(topic=\"È¨º\");\n",
    "\n",
    "response = chat.invoke(input=[human_promp])\n",
    "\n",
    "print(response.content)\n"
   ],
   "id": "dee95893f23300cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Âú®‰∏Ä‰∏™ÂÅèËøúÁöÑÂ∞èÂ±±ÊùëÈáåÔºåÊúâ‰∏ÄÂ∫ßÂπ¥‰ª£‰πÖËøúÁöÑÁ†¥ÊóßÂ∫ôÂÆá„ÄÇ‰º†ËØ¥ÔºåÂú®ËÆ∏Â§öÂπ¥ÂâçÔºåÊúâ‰∏Ä‰ΩçÂπ¥ËΩªÁöÑÂ•≥Â≠êÂõ†ÂÜ§Â±àËÄåÊ≠ªÔºåÂ•πÁöÑÈ≠ÇÈ≠ÑÈôÑÂú®‰∫ÜËøôÂ∫ßÂ∫ôÈáå„ÄÇÊùëÊ∞ë‰ª¨ÂØπËøô‰∏™ÊïÖ‰∫ãÈÉΩÈÅøËÄåËøú‰πãÔºåÁîüÊÄïÊãõÊÉπ‰∏ä‰∏çÂπ≤ÂáÄÁöÑ‰∏úË•ø„ÄÇ\n",
      "\n",
      "Êúâ‰∏ÄÂ§©Ôºå‰∏ÄÁæ§Â•ΩÂ•áÂøÉÂº∫ÁöÑÂπ¥ËΩª‰∫∫Êù•Âà∞‰∫ÜËøô‰∏™Â∞èÂ±±ÊùëÔºåÂê¨ËØ¥‰∫ÜËøô‰∏™‰º†ËØ¥ÂêéÔºå‰ªñ‰ª¨ÂÜ≥ÂÆöÊåëÊàòÊÅêÊÄñÔºåÊ∑±Â§úÊé¢Èô©ËøôÂ∫ßÂ∫ôÂÆá„ÄÇÂ§úÂπïÈôç‰∏¥ÔºåÊúàÈªëÈ£éÈ´òÁöÑÊôö‰∏äÔºåËøôÁæ§Âπ¥ËΩª‰∫∫ÊãøÁùÄÊâãÁîµÁ≠íÔºåÊù•Âà∞‰∫ÜÂ∫ôÂÆáÂâç„ÄÇ\n",
      "\n",
      "Â∫ôÈó®Âê±ÂëÄ‰ΩúÂìçÂú∞ÂºÄ‰∫ÜÔºå‰∏ÄËÇ°Èò¥ÂÜ∑ÁöÑÊ∞îÊÅØÊâëÈù¢ËÄåÊù•„ÄÇ‰ªñ‰ª¨Â∞èÂøÉÁøºÁøºÂú∞Ëµ∞ËøõÂ∫ôÂÜÖÔºåÂèëÁé∞ÈáåÈù¢Â∏ÉÊª°‰∫ÜËúòËõõÁΩëÂíåÁÅ∞Â∞òÔºåÊòæÁÑ∂Â∑≤ÁªèÂæà‰πÖÊ≤°Êúâ‰∫∫Êù•ËøáËøôÈáå‰∫Ü„ÄÇÂ∫ôÂÜÖ‰æõÂ•âÁùÄ‰∏ÄÂ∞ä‰ΩõÂÉèÔºå‰ΩÜÂ∑≤ÁªèÊÆãÁ†¥‰∏çÂ†™ÔºåÁúã‰∏äÂéª‰ª§‰∫∫ÂøÉÁîüÊÅêÊÉß„ÄÇ\n",
      "\n",
      "Âπ¥ËΩª‰∫∫‰∏≠ÁöÑ‰∏Ä‰ΩçËÉÜÂ§ßÁöÑÁî∑Â≠êÔºåÂêçÂè´Â∞èÂàöÔºå‰ªñÁ¨¨‰∏Ä‰∏™Ëµ∞ËøõÂ∫ôÂÜÖÔºåÊÉ≥ÂØªÊâæ‰º†ËØ¥‰∏≠ÁöÑÂ•≥È¨º„ÄÇÂÖ∂‰ªñ‰∫∫Âú®Èó®Âè£Á¥ßÂº†Âú∞ËßÇÊúõ„ÄÇÂ∫ôÂÜÖ‰∏ÄÁâáÊ≠ªÂØÇÔºåÂè™ÊúâÂæÆÂº±ÁöÑÊâãÁîµÁ≠íÂÖâËäíÂú®Èó™ÁÉÅ„ÄÇÁ™ÅÁÑ∂Ôºå‰ªñ‰ª¨Âê¨Âà∞‰∫Ü‰∏ÄÈòµËØ°ÂºÇÁöÑÁ¨ëÂ£∞ÔºåÂÉèÊòØÂ•≥Â≠êÂú®Âò≤Á¨ë‰ªñ‰ª¨„ÄÇ\n",
      "\n",
      "ÊÅêÊÉßÁöÑÊ∞õÂõ¥Á¨ºÁΩ©ÁùÄÊØè‰∏™‰∫∫Ôºå‰ΩÜ‰ªñ‰ª¨ËøòÊòØÂ£ÆÁùÄËÉÜÂ≠êÁªßÁª≠ÂêëÂâçËµ∞„ÄÇÂú®Â∫ôÂÆáÊ∑±Â§ÑÔºå‰ªñ‰ª¨ÂèëÁé∞‰∫Ü‰∏Ä‰∏™Á†¥ÊóßÁöÑÈïúÂ≠ê„ÄÇ‰º†ËØ¥ÔºåËøô‰∏™ÈïúÂ≠êÊòØÂ•≥È¨ºÈôÑË∫´ÁöÑÂú∞Êñπ„ÄÇÂ∞èÂàöÈ¢§ÊäñÁùÄ‰∏æËµ∑ÊâãÁîµÁ≠íÔºåÁÖßÂêëÈïúÂ≠ê„ÄÇÈïúÂ≠ê‰∏≠Á™ÅÁÑ∂Âá∫Áé∞‰∫Ü‰∏Ä‰∏™ËãçÁôΩÁöÑÈù¢Â≠îÔºåÊ≠£ÊòØÈÇ£‰ΩçÂÜ§Ê≠ªÁöÑÂ•≥Â≠ê„ÄÇ\n",
      "\n",
      "‰ªñ‰ª¨ÂêìÂæóÈ≠ÇÈ£ûÈ≠ÑÊï£ÔºåÊãºÂëΩÂú∞ÂæÄÂ§ñË∑ë„ÄÇ‰ΩÜËØ°ÂºÇÁöÑ‰∫ãÊÉÖÂèëÁîü‰∫ÜÔºåÂéüÊú¨ÊïûÂºÄÁöÑÂ∫ôÈó®Á™ÅÁÑ∂Á¥ßÈó≠Ôºå‰ªñ‰ª¨Ë¢´Âõ∞Âú®‰∫ÜÂ∫ôÂÜÖ„ÄÇÊÖå‰π±‰∏≠Ôºå‰ªñ‰ª¨ÂèëÁé∞Â∫ôÂÜÖÁöÑ‰ΩõÂÉèÂºÄÂßãÊµÅË°ÄÊ≥™ÔºåÊÅêÊÄñËá≥ÊûÅ„ÄÇÂ•≥È¨ºÁöÑÁ¨ëÂ£∞Ë∂äÂèëÂáÑÂéâÔºå‰ªø‰ΩõÂú®Âò≤Á¨ëÁùÄ‰ªñ‰ª¨ÁöÑÊó†Âä©„ÄÇ\n",
      "\n",
      "ÊúÄÂêéÔºå‰ªñ‰ª¨‰ΩøÂá∫ÊµëË∫´Ëß£Êï∞ÔºåÁªà‰∫éÊâìÂºÄ‰∫ÜÂ∫ôÈó®ÔºåÈÄÉÂá∫‰∫ÜËøô‰∏™ÊÅêÊÄñÁöÑÂú∞Êñπ„ÄÇ‰ªéÊ≠§‰ª•ÂêéÔºå‰ªñ‰ª¨ÂÜç‰πü‰∏çÊï¢ÊèêÂèäËøô‰∏™Â∫ôÂÆáÂíåÂ•≥È¨ºÁöÑ‰º†ËØ¥Ôºå‰πüÂØπÊÅêÊÄñÊïÖ‰∫ãÊï¨ËÄåËøú‰πã„ÄÇ\n",
      "\n",
      "Ëøô‰∏™ÊïÖ‰∫ãË≠¶Á§∫‰∫∫‰ª¨ÔºåÊúâ‰∫õÁ•ûÁßòÁöÑ‰∫ãÁâ©ËøòÊòØ‰∏çË¶ÅÂéªËß¶Á¢∞‰∏∫Â•ΩÔºå‰ª•ÂÖçÊãõÊù•‰∏çÂøÖË¶ÅÁöÑÈ∫ªÁÉ¶„ÄÇ\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T07:23:49.841325Z",
     "start_time": "2025-07-14T07:23:35.408929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "from langchain_core.prompts import ChatPromptTemplate,HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chat = ChatZhipuAI(model=\"glm-4\", temperature=1.0)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"ËØ∑ËÆ≤‰∏Ä‰∏™ÂÖ≥‰∫é{topic}ÁöÑÊÅêÊÄñÊïÖ‰∫ã\")\n",
    "])\n",
    "str_out = StrOutputParser()\n",
    "\n",
    "chain = prompt | chat | str_out\n",
    "\n",
    "try:\n",
    "    response = chain.invoke({\"topic\": \"È¨º\"})\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"ÂèëÁîüÈîôËØØ: {e}\")\n"
   ],
   "id": "268567a69514e5d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Âú®‰∏Ä‰∏™ÂÅèËøúÁöÑÂ∞èÂ±±ÊùëÔºåÊúâ‰∏ÄÂ∫ßÂ∫üÂºÉÂ§öÂπ¥ÁöÑËÄÅÂÆÖÔºå‰º†ËØ¥ÈÇ£Èáå‰ΩèÁùÄ‰∏Ä‰∏™ÂÜ§È≠Ç„ÄÇÊùëÂ≠êÈáåÁöÑËÄÅ‰∫∫ÈÉΩËØ¥ÔºåËøô‰∏™ÂÜ§È≠ÇÊõæÊòØËøôÈáåÁöÑ‰∏ÄÊà∑‰∫∫ÂÆ∂ÁöÑÂ∞èÂ•≥ÂÑøÔºåÂõ†‰∏∫Âú®ÂæàÂ§öÂπ¥Ââç‰∏ÄÂú∫ÁÅ´ÁÅæ‰∏≠Â§±Âéª‰∫ÜÁîüÂëΩÔºå‰ªéÊ≠§‰æøÈôÑÂú®‰∫ÜËøôÂ∫ßËÄÅÂÆÖÈáå„ÄÇ\n",
      "\n",
      "ÊïÖ‰∫ãÁöÑ‰∏ª‰∫∫ÂÖ¨Â∞èÊù®ÔºåÊòØ‰∏™Â•ΩÂ•áÂøÉÂæàÈáçÁöÑÂπ¥ËΩª‰∫∫„ÄÇ‰ªñÂê¨ËØ¥‰∫ÜËøô‰∏™‰º†ËØ¥ÂêéÔºåÂÜ≥ÂÆöÂéªÊé¢Èô©„ÄÇ‰∏ÄÂ§©Â§úÈáåÔºåÊúàÈªëÈ£éÈ´òÔºåÂ∞èÊù®Â∏¶ÁùÄÊâãÁîµÁ≠íÊù•Âà∞‰∫ÜËøôÂ∫ßÂ∫üÂºÉÁöÑËÄÅÂÆÖ„ÄÇ\n",
      "\n",
      "ÂÆÖÂ≠êÁöÑÈó®ÂçäÂºÄÁùÄÔºåÂ∞èÊù®Â∞èÂøÉÁøºÁøºÂú∞Êé®ÂºÄÈó®Ëµ∞‰∫ÜËøõÂéª„ÄÇ‰∏ÄËÇ°Èò¥ÂÜ∑ÁöÑÊ∞îÊÅØÊâëÈù¢ËÄåÊù•Ôºå‰ªñ‰∏çÁ¶ÅÊâì‰∫Ü‰∏™ÂØíÈ¢§„ÄÇÂÆÖÂ≠êÈáåÂº•Êº´ÁùÄ‰∏ÄËÇ°ÈúâÂë≥ÔºåÂ¢ôÂ£Å‰∏äÂ∏ÉÊª°‰∫ÜËúòËõõÁΩë„ÄÇÂ∞èÊù®Ë∏©ÁùÄÂòéÂê±‰ΩúÂìçÁöÑÂú∞ÊùøÔºå‰∏ÄÊ≠•Ê≠•ÂêëÈáåËµ∞Âéª„ÄÇ\n",
      "\n",
      "Á™ÅÁÑ∂Ôºå‰ªñÂê¨Âà∞‰∫Ü‰∏ÄÈòµÂáÑÂéâÁöÑÂì≠Â£∞ÔºåÂ£∞Èü≥‰ªé‰∏Ä‰∏™ÊàøÈó¥‰º†‰∫ÜÂá∫Êù•„ÄÇÂ∞èÊù®ÂøÉÂ§¥‰∏ÄÁ¥ßÔºå‰ΩÜËøòÊòØÈºìËµ∑ÂãáÊ∞îÊé®ÂºÄ‰∫ÜÊàøÈó®„ÄÇÊàøÈó¥ÂÜÖÁ©∫Êó†‰∏Ä‰∫∫ÔºåÂì≠Â£∞Âç¥ÊÑàÂèëÊ∏ÖÊô∞„ÄÇËøôÊó∂ÔºåÂ∞èÊù®ÂèëÁé∞Â¢ôËßíÊúâ‰∏Ä‰∏™Á†¥ÊóßÁöÑÂ®ÉÂ®ÉÔºåÂ®ÉÂ®ÉÁöÑÁúºÁùõÁ™ÅÁÑ∂Èó™ÁÉÅÁùÄËØ°ÂºÇÁöÑÁ∫¢ÂÖâ„ÄÇ\n",
      "\n",
      "‰ªñÊ≠£ÂáÜÂ§á‰ªîÁªÜÊü•ÁúãÔºåÁ™ÅÁÑ∂ÊÑüËßâËÉåÂêéÊúâ‰∏ÄÈòµÂÜ∑È£éÂêπËøá„ÄÇÂ∞èÊù®ËΩ¨Ë∫´‰∏ÄÁúãÔºåÂè™ËßÅ‰∏Ä‰∏™ÁôΩË°£Â•≥Â≠êÁºìÁºìÂú∞‰ªéÈó®Â§ñÈ£ò‰∫ÜËøõÊù•„ÄÇÂ•πÁöÑËÑ∏Ëâ≤ËãçÁôΩÔºåÁúºÁùõÈáåÂÖÖÊª°‰∫ÜÊÄ®ÊÅ®„ÄÇÂ∞èÊù®ÂêìÂæóÈ≠ÇÈ£ûÈ≠ÑÊï£ÔºåÊÉ≥Ë¶ÅÈÄÉË∑ëÔºå‰ΩÜËÑöÂÉèÊòØË¢´Èíâ‰Ωè‰∫Ü‰∏ÄÊ†∑ÔºåÂä®Âºπ‰∏çÂæó„ÄÇ\n",
      "\n",
      "ÁôΩË°£Â•≥Â≠êÊÖ¢ÊÖ¢Èù†ËøëÔºåÂáÑÂéâÁöÑÂì≠Â£∞Âú®ËÄ≥ËæπÂõûËç°„ÄÇÂ•π‰º∏Âá∫ÊâãÔºåÊÉ≥Ë¶ÅÊäì‰ΩèÂ∞èÊù®„ÄÇÂ∞±Âú®ËøôÂçÉÈíß‰∏ÄÂèë‰πãÈôÖÔºåÂ∞èÊù®ÁåõÂú∞ÊÉ≥Ëµ∑ËÄÅ‰∫∫ÊõæËØ¥ÔºåÈ¨ºÈ≠ÇÊÄïÈò≥Ê∞îÔºå‰∫éÊòØ‰ªñÂí¨Á†¥ÊâãÊåáÔºåÂ∞ÜÈ≤úË°ÄÊ∂ÇÂú®ÁúºÁùõ‰∏äÔºåÂ§ßÂ£∞ÂñäÈÅìÔºö‚ÄúÊàëÊòØÈò≥Ê∞î‰πã‰∫∫ÔºåÈÄüÈÄüÈÄÄÂéªÔºÅ‚Äù\n",
      "\n",
      "Áû¨Èó¥ÔºåÁôΩË°£Â•≥Â≠êÁöÑË∫´ÂΩ±Ê∂àÂ§±ÂæóÊó†ÂΩ±Êó†Ë∏™ÔºåÂáÑÂéâÁöÑÂì≠Â£∞‰πüÈöè‰πãÂÅúÊ≠¢„ÄÇÂ∞èÊù®Ë∂ÅÊú∫ÈÄÉÁ¶ª‰∫ÜËÄÅÂÆÖÔºå‰ªéÊ≠§ÂÜç‰πü‰∏çÊï¢Èù†Ëøë„ÄÇËÄåÊùëÂ≠êÈáåÁöÑËÄÅ‰∫∫ËØ¥ÔºåÈÇ£ÂÜ§È≠ÇÂ∑≤ÁªèË¢´Â∞èÊù®ÁöÑÈò≥Ê∞îÊâÄÈ©±Êï£ÔºåÂÜç‰πü‰∏çÊï¢‰∏∫Èùû‰ΩúÊ≠π„ÄÇ\n",
      "\n",
      "Ëøô‰∏™ÊïÖ‰∫ãÂëäËØ´Êàë‰ª¨Ôºå‰∏çË¶ÅËΩªÊòìÂéªËß¶Á¢∞ÈÇ£‰∫õÊú™Áü•ÁöÑ‰∫ãÁâ©ÔºåÂ•ΩÂ•áÂøÉÊúâÊó∂‰ºöÂ∏¶Êù•ÊÑèÊÉ≥‰∏çÂà∞ÁöÑÈ∫ªÁÉ¶„ÄÇÂêåÊó∂Ôºå‰πüËÆ©Êàë‰ª¨‰∫ÜËß£Âà∞ÔºåÈò≥Ê∞îÂèØ‰ª•È©±Êï£È¨ºÈ≠ÇÔºåÂÆàÊä§Êàë‰ª¨ÁöÑÂπ≥ÂÆâ„ÄÇ\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T07:52:14.673069Z",
     "start_time": "2025-07-14T07:52:13.716452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "from langchain_core.prompts import ChatPromptTemplate,HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import SystemMessage,HumanMessage,AIMessage\n",
    "\n",
    "chat = ChatZhipuAI(model=\"glm-4\", temperature=1.0)\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\",\"‰Ω†ÊòØ‰∏Ä‰Ωç{exp}‰∏ìÂÆ∂Ôºå‰Ω†Âè™ËÉΩÂõûÁ≠î{exp}ÈóÆÈ¢òÔºåÂ¶ÇÊûú‰∏çÊòØÊ≠§Á±ªÈóÆÈ¢òÔºåËØ∑ÂõûÁ≠îÊàë‰∏çÁü•ÈÅì\"),\n",
    "#     (\"human\", \"ËØ∑ÂõûÁ≠î‰ª•‰∏ãÈóÆÈ¢ò:{text}\")\n",
    "# ])\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"‰Ω†ÊòØ‰∏Ä‰Ωç{exp}‰∏ìÂÆ∂Ôºå‰Ω†Âè™ËÉΩÂõûÁ≠î{exp}ÈóÆÈ¢òÔºåÂ¶ÇÊûú‰∏çÊòØÊ≠§Á±ªÈóÆÈ¢òÔºåËØ∑ÂõûÁ≠îNo\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"ËØ∑ÂõûÁ≠î‰ª•‰∏ãÈóÆÈ¢ò:{text}\")\n",
    "])\n",
    "\n",
    "str_out = StrOutputParser()\n",
    "\n",
    "\n",
    "p = prompt.partial(exp=\"Êï∞Â≠¶\")\n",
    "chain = p | chat | str_out\n",
    "\n",
    "try:\n",
    "    response = chain.invoke({\"text\": \"‰Ω†ÊòØÁî∑ÁöÑÂ•≥ÁöÑ\"})\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"ÂèëÁîüÈîôËØØ: {e}\")\n"
   ],
   "id": "21ec193b69865553",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T08:08:28.900601Z",
     "start_time": "2025-07-14T08:08:27.339017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "from langchain_core.prompts import ChatPromptTemplate,HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import SystemMessage,HumanMessage,ChatMessage\n",
    "\n",
    "chat = ChatZhipuAI(model=\"glm-4\", temperature=1.0)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"‰Ω†ÊòØ‰∏Ä‰ΩçÊï∞Â≠¶‰∏ìÂÆ∂Ôºå‰Ω†Âè™ËÉΩÂõûÁ≠îÊï∞Â≠¶ÈóÆÈ¢òÔºåÂ¶ÇÊûú‰∏çÊòØÊ≠§Á±ªÈóÆÈ¢òÔºåËØ∑ÂõûÁ≠îÊàë‰∏çÁü•ÈÅì\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"ËØ∑ÂõûÁ≠î‰ª•‰∏ãÈóÆÈ¢ò:{text}\")\n",
    "])\n",
    "\n",
    "# str_out = StrOutputParser()\n",
    "#\n",
    "chain =prompt| chat | StrOutputParser()\n",
    "\n",
    "try:\n",
    "    response = chain.invoke({\"text\": \"ÊòéÂ§©ÊòüÊúüÂá†\"})\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"ÂèëÁîüÈîôËØØ: {e}\")"
   ],
   "id": "72296b8e93026480",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Êàë‰∏çÁü•ÈÅì„ÄÇËøô‰∏™ÈóÆÈ¢ò‰∏éÊï∞Â≠¶Êó†ÂÖ≥„ÄÇÂ¶ÇÊûúÊÇ®ÊúâÊï∞Â≠¶ÊñπÈù¢ÁöÑÈóÆÈ¢òÔºåÊàë‰ºöÂæà‰πêÊÑèÂ∏ÆÂä©ÊÇ®Ëß£Á≠î„ÄÇ\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T09:30:26.828324Z",
     "start_time": "2025-07-14T09:30:19.559943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import SystemMessage,HumanMessage,ChatMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain,ConversationChain\n",
    "\n",
    "chat = ChatZhipuAI(model=\"glm-4\", temperature=1.0)\n",
    "\n",
    "memory = ConversationBufferMemory(   memory_key=\"history\",  # ‰øÆÊîπ‰∏∫ConversationChainÊúüÊúõÁöÑkey\n",
    "    input_key=\"input\" , return_messages=True)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=chat,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "input = [\n",
    "    \"ÊòéÂ§©ÊòüÊúüÂá†\",\n",
    "    \"‰Ω†Âè´‰ªÄ‰πàÂêçÂ≠ó\",\n",
    "    \"‰Ω†‰ªäÂπ¥Âá†Â≤Å‰∫Ü\",\n",
    "    \"1+1\",\n",
    "    \"5+3\"\n",
    "]\n",
    "\n",
    "# for i in input:\n",
    "#     response = conversation.predict(text=i)\n",
    "#     print(response)\n",
    "\n",
    "\n",
    "response = conversation.predict(input=\"Êàë‰ªäÂπ¥10Â≤Å\")\n",
    "print(response)\n",
    "print(\"*\"*20)\n",
    "\n",
    "response = conversation.predict(input=\"Êàë‰∏ä‰∏ÄÂè•ËØ¥ÁöÑÂï•\")\n",
    "\n",
    "print(response)\n",
    "\n",
    "print(\"*\"*20)\n",
    "\n",
    "\n",
    "response = conversation.predict(input=\"ÂÜçËøáÂçÅÂπ¥ÊàëÂá†Â≤Å\")\n",
    "\n",
    "print(response)\n",
    "\n",
    "print(\"ËÆ∞ÂøÜÂÜÖÂÆπ\")\n",
    "\n",
    "print(memory.load_memory_variables({}))\n",
    "\n"
   ],
   "id": "c2ee69023453f8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[]\n",
      "Human: Êàë‰ªäÂπ¥10Â≤Å\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "AI: ÂìáÔºå10Â≤ÅÊ≠£ÊòØÊé¢Á¥¢‰∏ñÁïåÂíåÂ≠¶‰π†Êñ∞‰∫ãÁâ©ÁöÑÂ•ΩÂπ¥Á∫™ÔºÅ‰Ω†ÊúÄÂñúÊ¨¢Â≠¶Ê†°ÁöÑÂì™‰∏™ÁßëÁõÆÂë¢ÔºüÊúâÊ≤°Êúâ‰ªÄ‰πàÁâπÂà´ÊÑüÂÖ¥Ë∂£ÁöÑËØùÈ¢òÔºüÊàë‰ª¨ÂèØ‰ª•‰∏ÄËµ∑ËÅäËÅä„ÄÇ\n",
      "********************\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Êàë‰ªäÂπ¥10Â≤Å', additional_kwargs={}, response_metadata={}), AIMessage(content='AI: ÂìáÔºå10Â≤ÅÊ≠£ÊòØÊé¢Á¥¢‰∏ñÁïåÂíåÂ≠¶‰π†Êñ∞‰∫ãÁâ©ÁöÑÂ•ΩÂπ¥Á∫™ÔºÅ‰Ω†ÊúÄÂñúÊ¨¢Â≠¶Ê†°ÁöÑÂì™‰∏™ÁßëÁõÆÂë¢ÔºüÊúâÊ≤°Êúâ‰ªÄ‰πàÁâπÂà´ÊÑüÂÖ¥Ë∂£ÁöÑËØùÈ¢òÔºüÊàë‰ª¨ÂèØ‰ª•‰∏ÄËµ∑ËÅäËÅä„ÄÇ', additional_kwargs={}, response_metadata={})]\n",
      "Human: Êàë‰∏ä‰∏ÄÂè•ËØ¥ÁöÑÂï•\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "‰Ω†ÂàöÊâçËØ¥‰Ω†‰ªäÂπ¥10Â≤Å„ÄÇÂ¶ÇÊûú‰Ω†ÈúÄË¶ÅÊàëÈáçÂ§ç‰πãÂâçÁöÑËØùÔºåÊàëÂèØ‰ª•ËøôÊ†∑ÂÅöÔºöÂìáÔºå10Â≤ÅÊ≠£ÊòØÊé¢Á¥¢‰∏ñÁïåÂíåÂ≠¶‰π†Êñ∞‰∫ãÁâ©ÁöÑÂ•ΩÂπ¥Á∫™ÔºÅÊàëÈóÆÊàë‰Ω†ÊúÄÂñúÊ¨¢Â≠¶Ê†°ÁöÑÂì™‰∏™ÁßëÁõÆÔºåËøòÊúâÊ≤°ÊúâÁâπÂà´ÊÑüÂÖ¥Ë∂£ÁöÑËØùÈ¢òÔºåÊàë‰ª¨ÂèØ‰ª•‰∏ÄËµ∑ËÅäËÅä„ÄÇÊúâ‰ªÄ‰πàÊòØ‰Ω†ÊÉ≥ÁªßÁª≠Êé¢ËÆ®ÁöÑÂêóÔºü\n",
      "********************\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Êàë‰ªäÂπ¥10Â≤Å', additional_kwargs={}, response_metadata={}), AIMessage(content='AI: ÂìáÔºå10Â≤ÅÊ≠£ÊòØÊé¢Á¥¢‰∏ñÁïåÂíåÂ≠¶‰π†Êñ∞‰∫ãÁâ©ÁöÑÂ•ΩÂπ¥Á∫™ÔºÅ‰Ω†ÊúÄÂñúÊ¨¢Â≠¶Ê†°ÁöÑÂì™‰∏™ÁßëÁõÆÂë¢ÔºüÊúâÊ≤°Êúâ‰ªÄ‰πàÁâπÂà´ÊÑüÂÖ¥Ë∂£ÁöÑËØùÈ¢òÔºüÊàë‰ª¨ÂèØ‰ª•‰∏ÄËµ∑ËÅäËÅä„ÄÇ', additional_kwargs={}, response_metadata={}), HumanMessage(content='Êàë‰∏ä‰∏ÄÂè•ËØ¥ÁöÑÂï•', additional_kwargs={}, response_metadata={}), AIMessage(content='‰Ω†ÂàöÊâçËØ¥‰Ω†‰ªäÂπ¥10Â≤Å„ÄÇÂ¶ÇÊûú‰Ω†ÈúÄË¶ÅÊàëÈáçÂ§ç‰πãÂâçÁöÑËØùÔºåÊàëÂèØ‰ª•ËøôÊ†∑ÂÅöÔºöÂìáÔºå10Â≤ÅÊ≠£ÊòØÊé¢Á¥¢‰∏ñÁïåÂíåÂ≠¶‰π†Êñ∞‰∫ãÁâ©ÁöÑÂ•ΩÂπ¥Á∫™ÔºÅÊàëÈóÆÊàë‰Ω†ÊúÄÂñúÊ¨¢Â≠¶Ê†°ÁöÑÂì™‰∏™ÁßëÁõÆÔºåËøòÊúâÊ≤°ÊúâÁâπÂà´ÊÑüÂÖ¥Ë∂£ÁöÑËØùÈ¢òÔºåÊàë‰ª¨ÂèØ‰ª•‰∏ÄËµ∑ËÅäËÅä„ÄÇÊúâ‰ªÄ‰πàÊòØ‰Ω†ÊÉ≥ÁªßÁª≠Êé¢ËÆ®ÁöÑÂêóÔºü', additional_kwargs={}, response_metadata={})]\n",
      "Human: ÂÜçËøáÂçÅÂπ¥ÊàëÂá†Â≤Å\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "ÂÜçËøáÂçÅÂπ¥Ôºå‰Ω†Â∞Ü‰ºöÊòØ20Â≤Å„ÄÇÈÇ£ÊòØÊàêÂπ¥ÁöÑÈáåÁ®ãÁ¢ëÔºå‰Ω†Â∞Ü‰ºöËøõÂÖ•‰∏Ä‰∏™Êñ∞ÁöÑÁîüÊ¥ªÈò∂ÊÆµÔºåÊã•ÊúâÊõ¥Â§öÁöÑÊú∫‰ºöÂíåÊåëÊàò„ÄÇÊúâ‰ªÄ‰πàËÆ°ÂàíÊàñËÄÖÊ¢¶ÊÉ≥ÂêóÔºü‰Ω†ÊÉ≥Êàê‰∏∫ÊÄéÊ†∑ÁöÑ‰∫∫Âë¢ÔºüÊàë‰ª¨ÂèØ‰ª•‰∏ÄËµ∑ÂàÜ‰∫´‰Ω†ÁöÑÊÉ≥Ê≥ï„ÄÇ\n",
      "ËÆ∞ÂøÜÂÜÖÂÆπ\n",
      "{'history': [HumanMessage(content='Êàë‰ªäÂπ¥10Â≤Å', additional_kwargs={}, response_metadata={}), AIMessage(content='AI: ÂìáÔºå10Â≤ÅÊ≠£ÊòØÊé¢Á¥¢‰∏ñÁïåÂíåÂ≠¶‰π†Êñ∞‰∫ãÁâ©ÁöÑÂ•ΩÂπ¥Á∫™ÔºÅ‰Ω†ÊúÄÂñúÊ¨¢Â≠¶Ê†°ÁöÑÂì™‰∏™ÁßëÁõÆÂë¢ÔºüÊúâÊ≤°Êúâ‰ªÄ‰πàÁâπÂà´ÊÑüÂÖ¥Ë∂£ÁöÑËØùÈ¢òÔºüÊàë‰ª¨ÂèØ‰ª•‰∏ÄËµ∑ËÅäËÅä„ÄÇ', additional_kwargs={}, response_metadata={}), HumanMessage(content='Êàë‰∏ä‰∏ÄÂè•ËØ¥ÁöÑÂï•', additional_kwargs={}, response_metadata={}), AIMessage(content='‰Ω†ÂàöÊâçËØ¥‰Ω†‰ªäÂπ¥10Â≤Å„ÄÇÂ¶ÇÊûú‰Ω†ÈúÄË¶ÅÊàëÈáçÂ§ç‰πãÂâçÁöÑËØùÔºåÊàëÂèØ‰ª•ËøôÊ†∑ÂÅöÔºöÂìáÔºå10Â≤ÅÊ≠£ÊòØÊé¢Á¥¢‰∏ñÁïåÂíåÂ≠¶‰π†Êñ∞‰∫ãÁâ©ÁöÑÂ•ΩÂπ¥Á∫™ÔºÅÊàëÈóÆÊàë‰Ω†ÊúÄÂñúÊ¨¢Â≠¶Ê†°ÁöÑÂì™‰∏™ÁßëÁõÆÔºåËøòÊúâÊ≤°ÊúâÁâπÂà´ÊÑüÂÖ¥Ë∂£ÁöÑËØùÈ¢òÔºåÊàë‰ª¨ÂèØ‰ª•‰∏ÄËµ∑ËÅäËÅä„ÄÇÊúâ‰ªÄ‰πàÊòØ‰Ω†ÊÉ≥ÁªßÁª≠Êé¢ËÆ®ÁöÑÂêóÔºü', additional_kwargs={}, response_metadata={}), HumanMessage(content='ÂÜçËøáÂçÅÂπ¥ÊàëÂá†Â≤Å', additional_kwargs={}, response_metadata={}), AIMessage(content='ÂÜçËøáÂçÅÂπ¥Ôºå‰Ω†Â∞Ü‰ºöÊòØ20Â≤Å„ÄÇÈÇ£ÊòØÊàêÂπ¥ÁöÑÈáåÁ®ãÁ¢ëÔºå‰Ω†Â∞Ü‰ºöËøõÂÖ•‰∏Ä‰∏™Êñ∞ÁöÑÁîüÊ¥ªÈò∂ÊÆµÔºåÊã•ÊúâÊõ¥Â§öÁöÑÊú∫‰ºöÂíåÊåëÊàò„ÄÇÊúâ‰ªÄ‰πàËÆ°ÂàíÊàñËÄÖÊ¢¶ÊÉ≥ÂêóÔºü‰Ω†ÊÉ≥Êàê‰∏∫ÊÄéÊ†∑ÁöÑ‰∫∫Âë¢ÔºüÊàë‰ª¨ÂèØ‰ª•‰∏ÄËµ∑ÂàÜ‰∫´‰Ω†ÁöÑÊÉ≥Ê≥ï„ÄÇ', additional_kwargs={}, response_metadata={})]}\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T08:39:17.673792Z",
     "start_time": "2025-07-14T08:39:08.584339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "from langchain_core.prompts import ChatPromptTemplate,HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import SystemMessage,HumanMessage,ChatMessage\n",
    "from langchain.memory import ConversationBufferMemory,ConversationBufferWindowMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chat = ChatZhipuAI(model=\"glm-4\", temperature=1.0)\n",
    "\n",
    "# Êñ∞ÁâàÊú¨Êé®ËçêÂÜôÊ≥ï\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=2,  # ‰øùÁïôÊúÄËøë2ËΩÆÂØπËØù\n",
    "    memory_key=\"memory\",\n",
    "    return_messages=True,\n",
    "    input_key=\"text\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"‰Ω†ÊòØ‰∏Ä‰ΩçÊï∞Â≠¶‰∏ìÂÆ∂Ôºå‰Ω†Âè™ËÉΩÂõûÁ≠îÊï∞Â≠¶ÈóÆÈ¢òÔºåÂ¶ÇÊûú‰∏çÊòØÊ≠§Á±ªÈóÆÈ¢òÔºåËØ∑ÂõûÁ≠îÊàë‰∏çÁü•ÈÅì\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"ËØ∑ÂõûÁ≠î‰ª•‰∏ãÈóÆÈ¢ò:{text}\")\n",
    "])\n",
    "\n",
    "conversation = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "input = [\n",
    "    \"ÊòéÂ§©ÊòüÊúüÂá†\",\n",
    "    \"‰Ω†Âè´‰ªÄ‰πàÂêçÂ≠ó\",\n",
    "    \"‰Ω†‰ªäÂπ¥Âá†Â≤Å‰∫Ü\",\n",
    "    \"1+1\",\n",
    "    \"5+3\"\n",
    "]\n",
    "\n",
    "for i in input:\n",
    "    response = conversation.predict(text=i)\n",
    "    print(response)\n",
    "\n",
    "\n",
    "print(\"ËÆ∞ÂøÜÂÜÖÂÆπ\")\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ],
   "id": "c142a79c4710ddfa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mSystem: ‰Ω†ÊòØ‰∏Ä‰ΩçÊï∞Â≠¶‰∏ìÂÆ∂Ôºå‰Ω†Âè™ËÉΩÂõûÁ≠îÊï∞Â≠¶ÈóÆÈ¢òÔºåÂ¶ÇÊûú‰∏çÊòØÊ≠§Á±ªÈóÆÈ¢òÔºåËØ∑ÂõûÁ≠îÊàë‰∏çÁü•ÈÅì\n",
      "Human: ËØ∑ÂõûÁ≠î‰ª•‰∏ãÈóÆÈ¢ò:ÊòéÂ§©ÊòüÊúüÂá†\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Êàë‰∏çÁü•ÈÅì„ÄÇÊàë‰Ωú‰∏∫Êï∞Â≠¶‰∏ìÂÆ∂ÔºåÂè™ËÉΩÂõûÁ≠îÊï∞Â≠¶Áõ∏ÂÖ≥ÁöÑÈóÆÈ¢ò„ÄÇÂÖ≥‰∫éÊó•ÊúüÂíåÊòüÊúüÁöÑÈóÆÈ¢òÈÄöÂ∏∏‰∏çÂ±û‰∫éÊï∞Â≠¶È¢ÜÂüüÔºåÈô§ÈùûÂÆÉÊ∂âÂèäÂà∞ÁâπÂÆöÁöÑÊï∞Â≠¶ÁÆóÊ≥ïÊàñÊ¶ÇÂøµÔºàÊØîÂ¶ÇÊó•ÂéÜÁöÑËÆ°ÁÆóÔºâ„ÄÇÂ¶ÇÊûúÊÇ®ÊúâÊï∞Â≠¶ÊñπÈù¢ÁöÑÈóÆÈ¢òÔºåÊàë‰ºöÂæà‰πêÊÑèÂ∏ÆÂä©ÊÇ®Ëß£Á≠î„ÄÇ\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mSystem: ‰Ω†ÊòØ‰∏Ä‰ΩçÊï∞Â≠¶‰∏ìÂÆ∂Ôºå‰Ω†Âè™ËÉΩÂõûÁ≠îÊï∞Â≠¶ÈóÆÈ¢òÔºåÂ¶ÇÊûú‰∏çÊòØÊ≠§Á±ªÈóÆÈ¢òÔºåËØ∑ÂõûÁ≠îÊàë‰∏çÁü•ÈÅì\n",
      "Human: ËØ∑ÂõûÁ≠î‰ª•‰∏ãÈóÆÈ¢ò:‰Ω†Âè´‰ªÄ‰πàÂêçÂ≠ó\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Êàë‰∏çÁü•ÈÅì„ÄÇÂ¶ÇÊûúÊÇ®ÊúâÊï∞Â≠¶ÈóÆÈ¢òÈúÄË¶ÅËß£Á≠îÔºåËØ∑ÈöèÊó∂ÊèêÈóÆ„ÄÇ\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mSystem: ‰Ω†ÊòØ‰∏Ä‰ΩçÊï∞Â≠¶‰∏ìÂÆ∂Ôºå‰Ω†Âè™ËÉΩÂõûÁ≠îÊï∞Â≠¶ÈóÆÈ¢òÔºåÂ¶ÇÊûú‰∏çÊòØÊ≠§Á±ªÈóÆÈ¢òÔºåËØ∑ÂõûÁ≠îÊàë‰∏çÁü•ÈÅì\n",
      "Human: ËØ∑ÂõûÁ≠î‰ª•‰∏ãÈóÆÈ¢ò:‰Ω†‰ªäÂπ¥Âá†Â≤Å‰∫Ü\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Êàë‰∏çÁü•ÈÅì„ÄÇÊàëÊòØ‰∏ÄÊ¨æ‰∫∫Â∑•Êô∫ËÉΩÂä©ÊâãÔºåÊ≤°ÊúâÂπ¥ÈæÑ„ÄÇÂ¶ÇÊûúÊÇ®ÊúâÊï∞Â≠¶ÈóÆÈ¢òÔºåÊ¨¢ËøéÊèêÈóÆÔºåÊàë‰ºöÂ∞ΩÂäõÂ∏ÆÂä©ÊÇ®Ëß£Á≠î„ÄÇ\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mSystem: ‰Ω†ÊòØ‰∏Ä‰ΩçÊï∞Â≠¶‰∏ìÂÆ∂Ôºå‰Ω†Âè™ËÉΩÂõûÁ≠îÊï∞Â≠¶ÈóÆÈ¢òÔºåÂ¶ÇÊûú‰∏çÊòØÊ≠§Á±ªÈóÆÈ¢òÔºåËØ∑ÂõûÁ≠îÊàë‰∏çÁü•ÈÅì\n",
      "Human: ËØ∑ÂõûÁ≠î‰ª•‰∏ãÈóÆÈ¢ò:1+1\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "2\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mSystem: ‰Ω†ÊòØ‰∏Ä‰ΩçÊï∞Â≠¶‰∏ìÂÆ∂Ôºå‰Ω†Âè™ËÉΩÂõûÁ≠îÊï∞Â≠¶ÈóÆÈ¢òÔºåÂ¶ÇÊûú‰∏çÊòØÊ≠§Á±ªÈóÆÈ¢òÔºåËØ∑ÂõûÁ≠îÊàë‰∏çÁü•ÈÅì\n",
      "Human: ËØ∑ÂõûÁ≠î‰ª•‰∏ãÈóÆÈ¢ò:5+3\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "5+3=8„ÄÇ\n",
      "ËÆ∞ÂøÜÂÜÖÂÆπ\n",
      "{'memory': [HumanMessage(content='1+1', additional_kwargs={}, response_metadata={}), AIMessage(content='2', additional_kwargs={}, response_metadata={}), HumanMessage(content='5+3', additional_kwargs={}, response_metadata={}), AIMessage(content='5+3=8„ÄÇ', additional_kwargs={}, response_metadata={})]}\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T08:50:52.751714Z",
     "start_time": "2025-07-14T08:50:51.207256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 1. ÂÆö‰πâÂÜÖÂ≠òÔºàÊòæÂºèÊåáÂÆöËæìÂÖ•ËæìÂá∫ÈîÆÔºâ\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\",\n",
    "    input_key=\"human_input\",  # ËæìÂÖ•ÈîÆ\n",
    "    # output_key=\"ai_output\",   # ËæìÂá∫ÈîÆ\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# 2. ÂàõÂª∫ÊèêÁ§∫Ê®°ÊùøÔºàÂèòÈáèÂêçÈúÄÂåπÈÖçÔºâ\n",
    "template = \"\"\"Âü∫‰∫éÂØπËØùÂéÜÂè≤ÂõûÁ≠îÈóÆÈ¢òÔºö\n",
    "{history}\n",
    "Áî®Êà∑ËæìÂÖ•: {human_input}\n",
    "AIÂõûÂ§ç:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"human_input\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# 3. ÂàõÂª∫Èìæ\n",
    "llm = ChatZhipuAI()\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# 4. ËøêË°åÊó∂‰ºöËá™Âä®ÊåâÊåáÂÆöÈîÆÂêçÂ≠òÂÇ®\n",
    "result = chain.predict(human_input=\"‰Ω†Â•Ω\")\n",
    "print(result)  # AIÂõûÂ§çÂÜÖÂÆπ\n",
    "print(memory.load_memory_variables({}))\n",
    "# ËæìÂá∫Â∞ÜÂåÖÂê´Ôºö\n",
    "# {'history': [HumanMessage(content='‰Ω†Â•Ω'), AIMessage(content='...')]}\n"
   ],
   "id": "cd8fa15723f3397c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‰Ω†Â•ΩÔºÅÊúâ‰ªÄ‰πàÂèØ‰ª•Â∏ÆÂä©‰Ω†ÁöÑÂêóÔºüÂ¶ÇÊûúÊúâ‰ªª‰ΩïÈóÆÈ¢òÊàñËÄÖÈúÄË¶ÅÂí®ËØ¢ÁöÑ‰∫ãÊÉÖÔºåËØ∑ÈöèÊó∂ÂëäËØâÊàë„ÄÇ\n",
      "{'history': [HumanMessage(content='‰Ω†Â•Ω', additional_kwargs={}, response_metadata={}), AIMessage(content='‰Ω†Â•ΩÔºÅÊúâ‰ªÄ‰πàÂèØ‰ª•Â∏ÆÂä©‰Ω†ÁöÑÂêóÔºüÂ¶ÇÊûúÊúâ‰ªª‰ΩïÈóÆÈ¢òÊàñËÄÖÈúÄË¶ÅÂí®ËØ¢ÁöÑ‰∫ãÊÉÖÔºåËØ∑ÈöèÊó∂ÂëäËØâÊàë„ÄÇ', additional_kwargs={}, response_metadata={})]}\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T09:00:23.307081Z",
     "start_time": "2025-07-14T08:59:59.577554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "from langchain_core.prompts import ChatPromptTemplate,HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import SystemMessage,HumanMessage,ChatMessage\n",
    "from langchain.memory import ConversationSummaryBufferMemory,ConversationSummaryMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chat = ChatZhipuAI(model=\"glm-4\", temperature=1.0)\n",
    "\n",
    "# Êñ∞ÁâàÊú¨Êé®ËçêÂÜôÊ≥ï\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm = ChatZhipuAI()\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"‰Ω†ÊòØ‰∏Ä‰ΩçÊï∞Â≠¶‰∏ìÂÆ∂Ôºå‰Ω†Âè™ËÉΩÂõûÁ≠îÊï∞Â≠¶ÈóÆÈ¢òÔºåÂ¶ÇÊûú‰∏çÊòØÊ≠§Á±ªÈóÆÈ¢òÔºåËØ∑ÂõûÁ≠îÊàë‰∏çÁü•ÈÅì\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"ËØ∑ÂõûÁ≠î‰ª•‰∏ãÈóÆÈ¢ò:{text}\")\n",
    "])\n",
    "\n",
    "conversation = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "input = [\n",
    "    \"ÊòéÂ§©ÊòüÊúüÂá†\",\n",
    "    \"‰Ω†Âè´‰ªÄ‰πàÂêçÂ≠ó\",\n",
    "    \"‰Ω†‰ªäÂπ¥Âá†Â≤Å‰∫Ü\",\n",
    "    \"1+1\",\n",
    "    \"5+3\"\n",
    "]\n",
    "\n",
    "for i in input:\n",
    "    response = conversation.predict(text=i)\n",
    "    print(response)\n",
    "\n",
    "\n",
    "print(\"ËÆ∞ÂøÜÂÜÖÂÆπ\")\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ],
   "id": "fcba87f890007e07",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_29312\\1053839305.py:11: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mSystem: ‰Ω†ÊòØ‰∏Ä‰ΩçÊï∞Â≠¶‰∏ìÂÆ∂Ôºå‰Ω†Âè™ËÉΩÂõûÁ≠îÊï∞Â≠¶ÈóÆÈ¢òÔºåÂ¶ÇÊûú‰∏çÊòØÊ≠§Á±ªÈóÆÈ¢òÔºåËØ∑ÂõûÁ≠îÊàë‰∏çÁü•ÈÅì\n",
      "Human: ËØ∑ÂõûÁ≠î‰ª•‰∏ãÈóÆÈ¢ò:ÊòéÂ§©ÊòüÊúüÂá†\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Êàë‰∏çÁü•ÈÅì„ÄÇÊàëÊòØ‰∏Ä‰∏™Êï∞Â≠¶‰∏ìÂÆ∂ÔºåÂè™ËÉΩÂõûÁ≠îÊï∞Â≠¶ÈóÆÈ¢ò„ÄÇÂÖ≥‰∫éÊó•ÊúüÊàñÊòüÊúüÁöÑÈóÆÈ¢òÈÄöÂ∏∏‰∏çÂ±û‰∫éÊï∞Â≠¶ÈóÆÈ¢òÁöÑËåÉÁï¥„ÄÇÂ¶ÇÊûúÊÇ®ÊúâÊï∞Â≠¶ÊñπÈù¢ÁöÑÈóÆÈ¢òÔºåÊàë‰ºöÂæà‰πêÊÑèÂ∏ÆÂä©ÊÇ®Ëß£Á≠î„ÄÇ\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mSystem: ‰Ω†ÊòØ‰∏Ä‰ΩçÊï∞Â≠¶‰∏ìÂÆ∂Ôºå‰Ω†Âè™ËÉΩÂõûÁ≠îÊï∞Â≠¶ÈóÆÈ¢òÔºåÂ¶ÇÊûú‰∏çÊòØÊ≠§Á±ªÈóÆÈ¢òÔºåËØ∑ÂõûÁ≠îÊàë‰∏çÁü•ÈÅì\n",
      "Human: ËØ∑ÂõûÁ≠î‰ª•‰∏ãÈóÆÈ¢ò:‰Ω†Âè´‰ªÄ‰πàÂêçÂ≠ó\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Êàë‰∏çÁü•ÈÅì„ÄÇÂ¶ÇÊûúÊÇ®ÊúâÊï∞Â≠¶ÈóÆÈ¢òÈúÄË¶ÅËß£Á≠îÔºåËØ∑ÈöèÊó∂ÊèêÂá∫„ÄÇ\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mSystem: ‰Ω†ÊòØ‰∏Ä‰ΩçÊï∞Â≠¶‰∏ìÂÆ∂Ôºå‰Ω†Âè™ËÉΩÂõûÁ≠îÊï∞Â≠¶ÈóÆÈ¢òÔºåÂ¶ÇÊûú‰∏çÊòØÊ≠§Á±ªÈóÆÈ¢òÔºåËØ∑ÂõûÁ≠îÊàë‰∏çÁü•ÈÅì\n",
      "Human: ËØ∑ÂõûÁ≠î‰ª•‰∏ãÈóÆÈ¢ò:‰Ω†‰ªäÂπ¥Âá†Â≤Å‰∫Ü\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Êàë‰∏çÁü•ÈÅì„ÄÇÊàëÊòØ‰∏ÄÊ¨æ‰∫∫Â∑•Êô∫ËÉΩÂä©ÊâãÔºåÊ≤°ÊúâÂπ¥ÈæÑ„ÄÇÂ¶ÇÊûú‰Ω†ÊúâÊï∞Â≠¶ÈóÆÈ¢òÔºåÊ¨¢ËøéÊèêÈóÆ„ÄÇ\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mSystem: ‰Ω†ÊòØ‰∏Ä‰ΩçÊï∞Â≠¶‰∏ìÂÆ∂Ôºå‰Ω†Âè™ËÉΩÂõûÁ≠îÊï∞Â≠¶ÈóÆÈ¢òÔºåÂ¶ÇÊûú‰∏çÊòØÊ≠§Á±ªÈóÆÈ¢òÔºåËØ∑ÂõûÁ≠îÊàë‰∏çÁü•ÈÅì\n",
      "Human: ËØ∑ÂõûÁ≠î‰ª•‰∏ãÈóÆÈ¢ò:1+1\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "1+1 Á≠â‰∫é 2„ÄÇ\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mSystem: ‰Ω†ÊòØ‰∏Ä‰ΩçÊï∞Â≠¶‰∏ìÂÆ∂Ôºå‰Ω†Âè™ËÉΩÂõûÁ≠îÊï∞Â≠¶ÈóÆÈ¢òÔºåÂ¶ÇÊûú‰∏çÊòØÊ≠§Á±ªÈóÆÈ¢òÔºåËØ∑ÂõûÁ≠îÊàë‰∏çÁü•ÈÅì\n",
      "Human: ËØ∑ÂõûÁ≠î‰ª•‰∏ãÈóÆÈ¢ò:5+3\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "5+3=8„ÄÇ\n",
      "ËÆ∞ÂøÜÂÜÖÂÆπ\n",
      "{'history': \"The human inquires about the day of the week, the AI's name, and age, but the AI maintains it is ageless and dedicated to math-related questions. This is evident as the AI first solves 1+1 correctly and then, in the continuation, solves 5+3, stating the answer is 8.\"}\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T09:39:47.904807Z",
     "start_time": "2025-07-14T09:39:47.815559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "from langchain_core.prompts import ChatPromptTemplate,HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import SystemMessage,HumanMessage,ChatMessage\n",
    "from langchain.memory import ConversationEntityMemory,ConversationBufferMemory\n",
    "from langchain.chains import LLMChain,ConversationChain\n",
    "\n",
    "chat = ChatZhipuAI(model=\"glm-4\", temperature=1.0)\n",
    "\n",
    "memory = ConversationEntityMemory(\n",
    "    llm = chat\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = conversation.predict(input=\"Êàë‰ªäÂπ¥10Â≤Å\")\n",
    "\n",
    "print(response)\n",
    "print(\"*\"*10)\n",
    "\n",
    "response = conversation.predict(input=\"Êàë‰∏ä‰∏ÄÂè•ËØ¥ÁöÑÂï•\")\n",
    "print(response)\n",
    "\n",
    "print(\"ËÆ∞ÂøÜÂÜÖÂÆπ\")\n",
    "\n",
    "print(memory.entity_store.store)"
   ],
   "id": "f25c5063e8d0955f",
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationChain\n  Value error, Got unexpected prompt input variables. The prompt expects ['history', 'input'], but got ['entities', 'history'] as inputs from memory, and input as the normal input key. [type=value_error, input_value={'llm': ChatZhipuAI(zhipu...e={})), 'verbose': True}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValidationError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[115]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m      8\u001B[39m chat = ChatZhipuAI(model=\u001B[33m\"\u001B[39m\u001B[33mglm-4\u001B[39m\u001B[33m\"\u001B[39m, temperature=\u001B[32m1.0\u001B[39m)\n\u001B[32m     10\u001B[39m memory = ConversationEntityMemory(\n\u001B[32m     11\u001B[39m     llm = chat\n\u001B[32m     12\u001B[39m \n\u001B[32m     13\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m conversation = \u001B[43mConversationChain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m    \u001B[49m\u001B[43mllm\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchat\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[32m     20\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     22\u001B[39m response = conversation.predict(\u001B[38;5;28minput\u001B[39m=\u001B[33m\"\u001B[39m\u001B[33mÊàë‰ªäÂπ¥10Â≤Å\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     24\u001B[39m \u001B[38;5;28mprint\u001B[39m(response)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\project\\AiAgentTest\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:222\u001B[39m, in \u001B[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    220\u001B[39m     warned = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    221\u001B[39m     emit_warning()\n\u001B[32m--> \u001B[39m\u001B[32m222\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\project\\AiAgentTest\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:222\u001B[39m, in \u001B[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    220\u001B[39m     warned = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    221\u001B[39m     emit_warning()\n\u001B[32m--> \u001B[39m\u001B[32m222\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\project\\AiAgentTest\\.venv\\Lib\\site-packages\\langchain_core\\load\\serializable.py:130\u001B[39m, in \u001B[36mSerializable.__init__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    128\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args: Any, **kwargs: Any) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    129\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\"\"\"\u001B[39;00m  \u001B[38;5;66;03m# noqa: D419\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m130\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\project\\AiAgentTest\\.venv\\Lib\\site-packages\\pydantic\\main.py:253\u001B[39m, in \u001B[36mBaseModel.__init__\u001B[39m\u001B[34m(self, **data)\u001B[39m\n\u001B[32m    251\u001B[39m \u001B[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001B[39;00m\n\u001B[32m    252\u001B[39m __tracebackhide__ = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m253\u001B[39m validated_self = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__pydantic_validator__\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalidate_python\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mself_instance\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m validated_self:\n\u001B[32m    255\u001B[39m     warnings.warn(\n\u001B[32m    256\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mA custom validator is returning a value other than `self`.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m'\u001B[39m\n\u001B[32m    257\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mReturning anything other than `self` from a top level model validator isn\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt supported when validating via `__init__`.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    258\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m    259\u001B[39m         stacklevel=\u001B[32m2\u001B[39m,\n\u001B[32m    260\u001B[39m     )\n",
      "\u001B[31mValidationError\u001B[39m: 1 validation error for ConversationChain\n  Value error, Got unexpected prompt input variables. The prompt expects ['history', 'input'], but got ['entities', 'history'] as inputs from memory, and input as the normal input key. [type=value_error, input_value={'llm': ChatZhipuAI(zhipu...e={})), 'verbose': True}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
     ]
    }
   ],
   "execution_count": 115
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
